{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:23.698324Z",
     "iopub.status.busy": "2025-03-20T19:36:23.698033Z",
     "iopub.status.idle": "2025-03-20T19:36:26.647764Z",
     "shell.execute_reply": "2025-03-20T19:36:26.647150Z",
     "shell.execute_reply.started": "2025-03-20T19:36:23.698302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time, random, math, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:29.952642Z",
     "iopub.status.busy": "2025-03-20T19:36:29.952234Z",
     "iopub.status.idle": "2025-03-20T19:36:30.019723Z",
     "shell.execute_reply": "2025-03-20T19:36:30.018788Z",
     "shell.execute_reply.started": "2025-03-20T19:36:29.952617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6336\n",
    "dim = vocab_size\n",
    "batch_size = 64\n",
    "max_len = seq_len = 128\n",
    "hidden = 2048\n",
    "decoder = nn.TransformerDecoderLayer(d_model=512, nhead=8).to(device)\n",
    "\n",
    "memory = torch.rand(1, 1, 512).to(device)\n",
    "tgt = torch.rand(128, 1, 512).to(device)\n",
    "out = decoder(tgt, memory)\n",
    "print(out.shape)  # (128, 64, 6336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:31.653345Z",
     "iopub.status.busy": "2025-03-20T19:36:31.653040Z",
     "iopub.status.idle": "2025-03-20T19:36:31.657968Z",
     "shell.execute_reply": "2025-03-20T19:36:31.657225Z",
     "shell.execute_reply.started": "2025-03-20T19:36:31.653320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(device)\n",
    "DATA_PATH = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/dataset2.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:35.416832Z",
     "iopub.status.busy": "2025-03-20T19:36:35.416519Z",
     "iopub.status.idle": "2025-03-20T19:36:35.547876Z",
     "shell.execute_reply": "2025-03-20T19:36:35.547159Z",
     "shell.execute_reply.started": "2025-03-20T19:36:35.416791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input=DATA_PATH, model_prefix='tokenizer',\n",
    "                               vocab_size=316, model_type='word',\n",
    "                               unk_piece=\"<unk>\")  # Définit un token pour les mots inconnus\n",
    "\n",
    "\n",
    "# Load trained tokenizer\n",
    "sp = spm.SentencePieceProcessor(model_file='tokenizer.model')\n",
    "\n",
    "# Test tokenization\n",
    "print(sp.encode(\"Open Alacritty\", out_type=str))  # Example output: ['▁Open', '▁Alacritty']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = sp.pad_id()  # Récupérer l'ID du token de padding\n",
    "print(pad_token_id)\n",
    "print(sp.eos_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:33.629394Z",
     "iopub.status.busy": "2025-03-20T19:36:33.629123Z",
     "iopub.status.idle": "2025-03-20T19:36:33.655535Z",
     "shell.execute_reply": "2025-03-20T19:36:33.654732Z",
     "shell.execute_reply.started": "2025-03-20T19:36:33.629374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialisation des listes et dictionnaires\n",
    "l = []\n",
    "X = []\n",
    "Y = []\n",
    "word_to_int = {}\n",
    "int_to_word = {}\n",
    "current_index = 0\n",
    "\n",
    "# Lecture du fichier\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        l.append(line.strip())\n",
    "\n",
    "# Construction des dictionnaires\n",
    "for data in l:\n",
    "    split_data = data.split(\";\")\n",
    "    x = split_data[0]\n",
    "    y = split_data[1]\n",
    "\n",
    "    # Ajouter les mots au dictionnaire\n",
    "    for word in x.split() + y.split():\n",
    "        if word not in word_to_int:\n",
    "            word_to_int[word] = current_index+1\n",
    "            int_to_word[current_index +1 ] = word\n",
    "            current_index += 1\n",
    "\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "\n",
    "\n",
    "# Ajout du padding et conversion en tensors\n",
    "def pad_and_convert_to_tensor(sequences):\n",
    "    # Convertir les séquences en tensors\n",
    "    tensor_sequences = [torch.tensor(seq, dtype=torch.int64) for seq in sequences]\n",
    "    # Ajouter du padding\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=2)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:37.560522Z",
     "iopub.status.busy": "2025-03-20T19:36:37.560248Z",
     "iopub.status.idle": "2025-03-20T19:36:37.774050Z",
     "shell.execute_reply": "2025-03-20T19:36:37.773379Z",
     "shell.execute_reply.started": "2025-03-20T19:36:37.560500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encode dataset\n",
    "X_encoded = [[sp.bos_id()] + sp.encode(sentence, out_type=int) + [sp.eos_id()] for sentence in X]\n",
    "Y_encoded = [[sp.bos_id()] + sp.encode(sentence, out_type=int) + [sp.eos_id()] for sentence in Y]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:39.547017Z",
     "iopub.status.busy": "2025-03-20T19:36:39.546727Z",
     "iopub.status.idle": "2025-03-20T19:36:39.553567Z",
     "shell.execute_reply": "2025-03-20T19:36:39.552738Z",
     "shell.execute_reply.started": "2025-03-20T19:36:39.546996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(X_encoded[:10])\n",
    "print(sp.Decode(X_encoded[:10]))\n",
    "print()\n",
    "print(Y_encoded[:10])\n",
    "print(sp.Decode(Y_encoded[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:43.096288Z",
     "iopub.status.busy": "2025-03-20T19:36:43.096009Z",
     "iopub.status.idle": "2025-03-20T19:36:43.447620Z",
     "shell.execute_reply": "2025-03-20T19:36:43.446755Z",
     "shell.execute_reply.started": "2025-03-20T19:36:43.096267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_Tensor = pad_and_convert_to_tensor(X_encoded)\n",
    "Y_Tensor = pad_and_convert_to_tensor(Y_encoded)\n",
    "\n",
    "print(X_Tensor.shape)\n",
    "print(Y_Tensor.shape)\n",
    "\n",
    "dataset = list(zip(X_Tensor.to(device), Y_Tensor.to(device)))\n",
    "#dataset = random.sample(dataset, len(dataset))\n",
    "\n",
    "# Mélange du dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "def create_batches(dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Divise le dataset en batches de taille batch_size.\n",
    "\n",
    "    Args:\n",
    "    - dataset (list): Liste de tuples (X, Y).\n",
    "    - batch_size (int): Taille de chaque batch.\n",
    "\n",
    "    Returns:\n",
    "    - List[List]: Liste de batches.\n",
    "    \"\"\"\n",
    "    batches = [dataset[i:i + batch_size] for i in range(0, len(dataset), batch_size)]\n",
    "    return batches\n",
    "\n",
    "# Division du dataset en ensembles d'entraînement et de test\n",
    "test_dataset = dataset[int(0.8 * len(dataset)):]\n",
    "train_dataset = dataset[:int(0.8 * len(dataset))]\n",
    "\n",
    "# Création des batches pour l'ensemble d'entraînement\n",
    "batch_size = 32\n",
    "train_batches = create_batches(train_dataset, batch_size)\n",
    "\n",
    "# Affichage des informations\n",
    "print(len(train_dataset))\n",
    "print(\"Nombre total de batches d'entraînement:\", len(train_batches))\n",
    "print(train_batches[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:45.565895Z",
     "iopub.status.busy": "2025-03-20T19:36:45.565564Z",
     "iopub.status.idle": "2025-03-20T19:36:45.571688Z",
     "shell.execute_reply": "2025-03-20T19:36:45.571059Z",
     "shell.execute_reply.started": "2025-03-20T19:36:45.565865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_dataset[0][0])\n",
    "print()\n",
    "print(train_batches[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de FastText (choisir un modèle préentraîné)\n",
    "ft_model = fasttext.load_model(\"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/cc en 300.bin\")  # Modèle anglais\n",
    "\n",
    "# Créer une matrice d'embeddings avec la même taille que le vocabulaire\n",
    "embedding_dim = 300\n",
    "vocab_size = len(sp)  # Taille du vocabulaire de SentencePiece\n",
    "embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Remplir la matrice avec les vecteurs des mots connus\n",
    "for i in range(vocab_size):\n",
    "    word = sp.id_to_piece(i)\n",
    "    embedding_matrix[i] = torch.tensor(ft_model.get_word_vector(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:50.788333Z",
     "iopub.status.busy": "2025-03-20T19:36:50.788053Z",
     "iopub.status.idle": "2025-03-20T19:36:50.792472Z",
     "shell.execute_reply": "2025-03-20T19:36:50.791582Z",
     "shell.execute_reply.started": "2025-03-20T19:36:50.788312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:36:58.195488Z",
     "iopub.status.busy": "2025-03-20T19:36:58.195217Z",
     "iopub.status.idle": "2025-03-20T19:36:58.199223Z",
     "shell.execute_reply": "2025-03-20T19:36:58.198380Z",
     "shell.execute_reply.started": "2025-03-20T19:36:58.195467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "MAX_LENGTH = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement du model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)  # Couche linéaire pour les poids\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))  # Paramètre appris\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # Appliquer une attention entre l'état du décodeur et chaque état de l'encodeur\n",
    "        scores = torch.matmul(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)  # (batch, seq_len)\n",
    "\n",
    "        # Appliquer un softmax pour obtenir les poids d’attention\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (batch, seq_len)\n",
    "\n",
    "        # Calcul du contexte : somme pondérée des états de l’encodeur\n",
    "        context_vector = torch.sum(attn_weights.unsqueeze(2) * encoder_outputs, dim=1)  # (batch, hidden_size)\n",
    "\n",
    "        return context_vector, attn_weights  # (batch_size, hidden_dim), (batch_size, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:38:10.378681Z",
     "iopub.status.busy": "2025-03-20T19:38:10.378259Z",
     "iopub.status.idle": "2025-03-20T19:38:10.383658Z",
     "shell.execute_reply": "2025-03-20T19:38:10.382763Z",
     "shell.execute_reply.started": "2025-03-20T19:38:10.378652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src):\n",
    "        # src : [sen_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        # embedded : [sen_len, batch_size, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n",
    "        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_direction, batch_size, hid_dim]\n",
    "        return hidden, cell, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:38:11.829682Z",
     "iopub.status.busy": "2025-03-20T19:38:11.829382Z",
     "iopub.status.idle": "2025-03-20T19:38:11.835281Z",
     "shell.execute_reply": "2025-03-20T19:38:11.834430Z",
     "shell.execute_reply.started": "2025-03-20T19:38:11.829656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(2*hid_dim, output_dim)\n",
    "        self.fc_projection = nn.Linear(521, 2*hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \n",
    "        # input = [batch_size]\n",
    "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        # input : [1, ,batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        rnn_output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq_len, batch_size, hid_dim * n_dir]\n",
    "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
    "\n",
    "        # Appliquer l’attention de Luong\n",
    "        context_vector, _ = self.attention(rnn_output.squeeze(0), encoder_outputs)\n",
    "\n",
    "        #print(\"rnn_out\",rnn_output.shape)\n",
    "        context_vector = context_vector.mean(dim=1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        \n",
    "        cat = torch.cat((rnn_output.squeeze(0), context_vector.unsqueeze(0)), dim=-1)\n",
    "        cat = self.fc_projection(cat)\n",
    "\n",
    "        #print(\"cat\" ,cat.shape)\n",
    "\n",
    "        # Combiner la sortie du RNN et le vecteur de contexte\n",
    "        prediction = self.fc_out(cat)\n",
    "        #print(\"pred\", prediction.shape)\n",
    "        \n",
    "        # seq_len and n_dir will always be 1 in the decoder\n",
    "        #prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:38:13.931012Z",
     "iopub.status.busy": "2025-03-20T19:38:13.930685Z",
     "iopub.status.idle": "2025-03-20T19:38:13.937484Z",
     "shell.execute_reply": "2025-03-20T19:38:13.936508Z",
     "shell.execute_reply.started": "2025-03-20T19:38:13.930983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            'hidden dimensions of encoder and decoder must be equal.'\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'n_layers of encoder and decoder must be equal.'\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        src = src.to(self.device)\n",
    "        trg = trg.to(self.device)\n",
    "        # src = [sen_len, batch_size]\n",
    "        # trg = [sen_len, batch_size]\n",
    "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell, output_encoder = self.encoder(src)\n",
    "        \n",
    "        # first input to the decoder is the <sos> token.\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden and previous cell states \n",
    "            # receive output tensor (predictions) and new hidden and cell states.\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell,  output_encoder)\n",
    "            #print(\"output_encoder\",output_encoder.shape)\n",
    "            # replace predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not.\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # get the highest predicted token from our predictions.\n",
    "            top1 = output.argmax(1)\n",
    "            # update input : use ground_truth when teacher_force \n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:37:31.803625Z",
     "iopub.status.busy": "2025-03-20T19:37:31.803344Z",
     "iopub.status.idle": "2025-03-20T19:37:31.807743Z",
     "shell.execute_reply": "2025-03-20T19:37:31.807032Z",
     "shell.execute_reply.started": "2025-03-20T19:37:31.803603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T20:07:49.828856Z",
     "iopub.status.busy": "2025-03-20T20:07:49.828520Z",
     "iopub.status.idle": "2025-03-20T20:07:49.870929Z",
     "shell.execute_reply": "2025-03-20T20:07:49.870210Z",
     "shell.execute_reply.started": "2025-03-20T20:07:49.828800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_dim = vocab_size\n",
    "out_dim = vocab_size\n",
    "emb_dim = 300\n",
    "hid_dim = 512\n",
    "n_layers = 4\n",
    "dropout = 0.3\n",
    "\n",
    "attention = AttentionBlock(hid_dim)\n",
    "encoder = Encoder(input_dim, emb_dim, hid_dim, n_layers, dropout).to(device)\n",
    "decoder = Decoder(out_dim, emb_dim, hid_dim, n_layers, dropout, attention).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "# xt : [sen_len, batch_size]\n",
    "xt = train_dataset[0][0]\n",
    "x_t = xt.unsqueeze(-1)\n",
    "yt = train_dataset[0][1]\n",
    "y_t = yt.unsqueeze(-1)\n",
    "print(yt)\n",
    "print(sp.decode(xt.tolist()))\n",
    "print(sp.decode(yt.tolist()))\n",
    "h, c, o = encoder(x_t)\n",
    "print(h.shape)\n",
    "print(c.shape)\n",
    "z_t = model(x_t.to(device), y_t.to(device))\n",
    "predicted_indices = z_t.argmax(dim=-1)  # [trg_len, batch_size]\n",
    "print(z_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:42:37.729396Z",
     "iopub.status.busy": "2025-03-20T19:42:37.729117Z",
     "iopub.status.idle": "2025-03-20T19:42:37.736906Z",
     "shell.execute_reply": "2025-03-20T19:42:37.736063Z",
     "shell.execute_reply.started": "2025-03-20T19:42:37.729375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:42:40.119085Z",
     "iopub.status.busy": "2025-03-20T19:42:40.118782Z",
     "iopub.status.idle": "2025-03-20T19:42:40.123168Z",
     "shell.execute_reply": "2025-03-20T19:42:40.122118Z",
     "shell.execute_reply.started": "2025-03-20T19:42:40.119061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:38:27.767490Z",
     "iopub.status.busy": "2025-03-20T19:38:27.767173Z",
     "iopub.status.idle": "2025-03-20T19:38:27.772767Z",
     "shell.execute_reply": "2025-03-20T19:38:27.771933Z",
     "shell.execute_reply.started": "2025-03-20T19:38:27.767462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (X, Y) in enumerate(iterator):\n",
    "        src = X.unsqueeze(-1)\n",
    "        trg = Y.unsqueeze(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # trg = [sen_len, batch_size]\n",
    "        # output = [trg_len, batch_size, output_dim]\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n",
    "        output = output[1:].view(-1, output_dim) \n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg_len-1) * batch_size]\n",
    "        # output = [(trg_len-1) * batch_size, output_dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:38:29.805591Z",
     "iopub.status.busy": "2025-03-20T19:38:29.805289Z",
     "iopub.status.idle": "2025-03-20T19:38:29.810622Z",
     "shell.execute_reply": "2025-03-20T19:38:29.809923Z",
     "shell.execute_reply.started": "2025-03-20T19:38:29.805559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, (X, Y) in enumerate(iterator):\n",
    "            src = X.unsqueeze(-1)\n",
    "            trg = Y.unsqueeze(-1)\n",
    "            \n",
    "            output = model(src, trg, 0) # turn off teacher forcing.\n",
    "            \n",
    "            # trg = [sen_len, batch_size]\n",
    "            # output = [sen_len, batch_size, output_dim]\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T19:38:32.535439Z",
     "iopub.status.busy": "2025-03-20T19:38:32.535142Z",
     "iopub.status.idle": "2025-03-20T19:38:32.539262Z",
     "shell.execute_reply": "2025-03-20T19:38:32.538439Z",
     "shell.execute_reply.started": "2025-03-20T19:38:32.535414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# a function that used to tell us how long an epoch takes.\n",
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time  / 60)\n",
    "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
    "    return  elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T20:33:20.870119Z",
     "iopub.status.busy": "2025-03-20T20:33:20.869837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, test_dataset, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'Seq2SeqModel.pt')\n",
    "    print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test du model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T20:11:06.409358Z",
     "iopub.status.busy": "2025-03-20T20:11:06.409079Z",
     "iopub.status.idle": "2025-03-20T20:11:06.425123Z",
     "shell.execute_reply": "2025-03-20T20:11:06.424228Z",
     "shell.execute_reply.started": "2025-03-20T20:11:06.409338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/Seq2SeqModel.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T20:30:33.182080Z",
     "iopub.status.busy": "2025-03-20T20:30:33.181757Z",
     "iopub.status.idle": "2025-03-20T20:30:33.187141Z",
     "shell.execute_reply": "2025-03-20T20:30:33.186272Z",
     "shell.execute_reply.started": "2025-03-20T20:30:33.182056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference(model, src, max_len, start_token, end_token):\n",
    "    src = src.to(model.device)\n",
    "    hidden, cell = model.encoder(src)\n",
    "\n",
    "    # Initialisation du premier token avec <SOS>\n",
    "    inputs = torch.tensor([start_token]).to(model.device)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        output, hidden, cell = model.decoder(inputs, hidden, cell)\n",
    "        token = output.argmax(dim=-1)  # Prédiction du mot avec la plus grande probabilité\n",
    "        outputs.append(token.item())\n",
    "\n",
    "        # Arrêter si on atteint le token de fin (<EOS>)\n",
    "        if token.item() == end_token:\n",
    "            break\n",
    "\n",
    "        # Réinjecter la prédiction comme entrée\n",
    "        input = token.unsqueeze(0)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T20:32:51.450430Z",
     "iopub.status.busy": "2025-03-20T20:32:51.450043Z",
     "iopub.status.idle": "2025-03-20T20:32:51.456261Z",
     "shell.execute_reply": "2025-03-20T20:32:51.455320Z",
     "shell.execute_reply.started": "2025-03-20T20:32:51.450396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_action(sentence, model, max_len, start_token=SOS_token, end_token=6):\n",
    "    # convert sentence to int then to tensor (with padding)\n",
    "    x = sp.Encode([sentence], out_type=int)\n",
    "    xt = pad_and_convert_to_tensor(x) # input size : [1, sentence lenght]\n",
    "    xt = xt.permute(1, 0) # input size : [sentence lenght, 1]\n",
    "    print(xt.shape)\n",
    "    action = inference(model, xt, max_len, start_token, end_token)\n",
    "    return sp.Decode(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T20:29:55.297704Z",
     "iopub.status.busy": "2025-03-20T20:29:55.297422Z",
     "iopub.status.idle": "2025-03-20T20:29:55.312836Z",
     "shell.execute_reply": "2025-03-20T20:29:55.312182Z",
     "shell.execute_reply.started": "2025-03-20T20:29:55.297682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = inference(model, x_t, 17, 1, 6)\n",
    "print(x_t.shape)\n",
    "print(y)\n",
    "print(sp.Decode(xt.tolist()))\n",
    "print(sp.Decode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T20:33:15.438374Z",
     "iopub.status.busy": "2025-03-20T20:33:15.438084Z",
     "iopub.status.idle": "2025-03-20T20:33:15.452591Z",
     "shell.execute_reply": "2025-03-20T20:33:15.451881Z",
     "shell.execute_reply.started": "2025-03-20T20:33:15.438341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "action = generate_action(\"Search for something on Google\", model, 17)\n",
    "print(action)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6854347,
     "sourceId": 11009552,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
