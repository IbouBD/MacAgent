{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10af846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4254ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=50):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bbe7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87853d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/vocab/instruction.txt\"\n",
    "model_path = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/TD/actor4.pth\"\n",
    "vision_path = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/TD/mousse_net.pth\"\n",
    "id_to_action_path = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/notebook/id_to_action.json\"\n",
    "action_to_id_path = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/notebook/action_to_id.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc256c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(id_to_action_path) as f:\n",
    "    id_to_action_raw = json.load(f)\n",
    "\n",
    "id_to_action = {int(k): v for k, v in id_to_action_raw.items()}\n",
    "\n",
    "with open(action_to_id_path) as f:\n",
    "    action_to_id_raw = json.load(f)\n",
    "\n",
    "action_to_id = {k: int(v) for k, v in action_to_id_raw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b938080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1949\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(action_to_id)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1cc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b3bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b176671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 448 # 448 est peut etre la meilleur taille pour l'instant mais 896 fait le taffe également"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e662a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image in grayscale\n",
    "path = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/TD/image.png\"\n",
    "# 1. Charger l'image en niveaux de gris (1 canal)\n",
    "img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 2. Prétraitement\n",
    "blur = cv2.GaussianBlur(img, (5, 5), 1.4)\n",
    "edges = cv2.Canny(blur, threshold1=100, threshold2=200)  # Utiliser blur pour de meilleurs résultats\n",
    "edges = cv2.resize(edges, (IMG_SIZE, IMG_SIZE))  # Taille attendue par MouseNet (64x64)\n",
    "\n",
    "# 3. Ajouter les dimensions manquantes\n",
    "# - Convertir en float32 et normaliser [0, 255] -> [0, 1]\n",
    "edges = edges.astype(np.float32) / 255.0\n",
    "\n",
    "# - Ajouter les dimensions: [Hauteur, Largeur] -> [Canaux, Hauteur, Largeur]\n",
    "edges = np.expand_dims(edges, axis=0)  # Maintenant shape (1, 64, 64)\n",
    " \n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "img_arr = torch.tensor(edges, dtype=torch.float32).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98ef48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=2):\n",
    "        super(ResidualFFN, self).__init__()\n",
    "        \n",
    "        # Projection initiale\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Blocs résiduels\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Projection finale\n",
    "        self.output_proj = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Appliquer les blocs résiduels\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        return self.output_proj(x)\n",
    "        \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.layers(x))\n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, dim)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, encoder, dim, hidden, vocab_size, max_len=128):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder  # pretrained SentenceTransformer\n",
    "        self.rffn = ResidualFFN(384, hidden, dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_encoding = PositionalEncoding(dim, max_len=max_len)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=dim, nhead=16, dim_feedforward=hidden, dropout=0.3)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=2)  # Réduction de 6 à 2 couches\n",
    "        self.final_projection = nn.Linear(dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        self.dim = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def forward(self, x_texts, tgt):\n",
    "        \"\"\"\n",
    "        x_texts: list of strings, len = batch_size\n",
    "        tgt: tensor of shape (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = len(x_texts)\n",
    "        \n",
    "        # Encode input texts\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder.encode(x_texts, convert_to_tensor=True)  # shape: (batch_size, 384)\n",
    "        x = self.rffn(x)  # shape: (batch_size, dim)\n",
    "       \n",
    "\n",
    "        # Prepare target sequence\n",
    "        tgt = tgt.to(device)\n",
    "        tgt = self.embedding(tgt)  # (batch_size, seq_len, dim)\n",
    "        tgt = tgt.permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "        tgt = self.pos_encoding(tgt)  # add positional encoding\n",
    "\n",
    "        # Create mask for autoregressive decoding\n",
    "        seq_len = tgt.size(0)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(tgt.device)\n",
    "        x = x.unsqueeze(0).repeat(seq_len, 1, 1)  # (seq_len, batch_size, dim)\n",
    "        # Decode\n",
    "        z = self.transformer_decoder(tgt, x, tgt_mask=tgt_mask)  # (seq_len, batch_size, dim)\n",
    "        z = self.final_projection(z)  # (seq_len, batch_size, vocab_size)\n",
    "        z = z.permute(1, 0, 2)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def forward_training(self, x, tgt):\n",
    "        \"\"\"\n",
    "        x: encoder output (batch_size, dim)\n",
    "        tgt: tensor of shape (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # Projette x dans le bon espace si nécessaire\n",
    "        x = self.rffn(x)  # (batch_size, dim)\n",
    "\n",
    "        # Embedding + Positional encoding\n",
    "        tgt = self.embedding(tgt)  # (batch_size, seq_len, dim)\n",
    "        tgt = tgt.permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "\n",
    "        # Memory (encoder output) doit être (seq_len_enc, batch_size, dim)\n",
    "        # Ici on suppose x est global, donc on le répète\n",
    "        x = x.unsqueeze(0)  # (1, batch_size, dim)\n",
    "\n",
    "        # Masque auto-régressif pour le décodeur\n",
    "        seq_len = tgt.size(0)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(tgt.device)\n",
    "\n",
    "        # Transformer decoder\n",
    "        z = self.transformer_decoder(tgt, x, tgt_mask=tgt_mask)  # (seq_len, batch_size, dim)\n",
    "        z = self.final_projection(z)  # (seq_len, batch_size, vocab_size)\n",
    "        z = z.permute(1, 0, 2)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return z\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, x_text:list[str], max_len=32, start_token_id=1, end_token_id=2):\n",
    "        \"\"\"\n",
    "        x_text : liste de string\n",
    "        Retourne une liste de listes contenant les ID générés\n",
    "        \"\"\"\n",
    "        # Encode input texts\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder.encode(x_text, convert_to_tensor=True)\n",
    "        # Encoder: passe par rffn si nécessaire\n",
    "        x = self.rffn(x)  # (batch_size, dim)\n",
    "        memory = x.unsqueeze(0)  # (1, batch_size, dim)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        device = x.device\n",
    "\n",
    "        # Initialiser avec <BOS>\n",
    "        generated = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Embed + position\n",
    "            tgt_embed = self.embedding(generated)  # (batch_size, seq_len, dim)\n",
    "            tgt_embed = tgt_embed.permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "            tgt_embed = self.pos_encoding(tgt_embed)\n",
    "\n",
    "            # Masque causal\n",
    "            seq_len = generated.size(1)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
    "\n",
    "            # Decode\n",
    "            output = self.transformer_decoder(tgt_embed, memory, tgt_mask=tgt_mask)\n",
    "            logits = self.final_projection(output)  # (seq_len, batch_size, vocab_size)\n",
    "            next_token_logits = logits[-1, :, :]  # dernier pas de temps → (batch_size, vocab_size)\n",
    "\n",
    "            # Greedy : choisir l'indice du max\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "            # Ajouter à la séquence\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "            # Option d'arrêt : si tous les batchs ont généré <EOS>\n",
    "            if (next_token == end_token_id).all():\n",
    "                break\n",
    "\n",
    "        return generated  # (batch_size, seq_len_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdef06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MouseNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=2, input_size=IMG_SIZE):\n",
    "        super(MouseNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Couches convolutionnelles\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Calcul de la taille après convolutions\n",
    "        # input_size -> /2 -> /2 = input_size // 4\n",
    "        self.feature_size = self._get_conv_output_size(in_channels, input_size)\n",
    "        \n",
    "        # Couches fully connected\n",
    "        self.fc1 = nn.Linear(self.feature_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.out = nn.Linear(128, num_classes)\n",
    "        self.load_state_dict(torch.load(vision_path))\n",
    "        \n",
    "    def _get_conv_output_size(self, in_channels, input_size):\n",
    "        \"\"\"Calcule la taille de sortie des couches convolutionnelles\"\"\"\n",
    "        # Simulation d'un passage dans les conv layers\n",
    "        x = torch.randn(1, in_channels, input_size, input_size)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x, features_only=False):\n",
    "        # x shape attendue: (batch_size, in_channels, height, width)\n",
    "        print(\"x:\", x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Couches convolutionnelles\n",
    "        x = F.relu(self.conv1(x))  # (batch, 8, H, W)\n",
    "        x = self.pool(x)           # (batch, 8, H/2, W/2)\n",
    "        x = F.relu(self.conv2(x))  # (batch, 16, H/2, W/2)\n",
    "        x = self.pool(x)           # (batch, 16, H/4, W/4)\n",
    "        x = F.relu(self.conv3(x))  # (batch, 32, H/4, W/4)\n",
    "        \n",
    "        # Aplatissement pour les couches fully connected\n",
    "        x = x.view(batch_size, -1)  # (batch, 32 * H/4 * W/4)\n",
    "        \n",
    "        # Couches fully connected\n",
    "        x = F.relu(self.fc1(x))    # (batch, 64)\n",
    "        x = F.relu(self.fc2(x))    # (batch, 128)\n",
    "        if features_only==True:\n",
    "            return x\n",
    "        else:\n",
    "            x = self.out(x)            # (batch, num_classes)\n",
    "            return torch.sigmoid(x).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9a47ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionActor(nn.Module):\n",
    "    def __init__(self, in_channels, encoder, dim, hidden, vocab_size):\n",
    "        super(VisionActor, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.encoder = encoder\n",
    "        self.dim = dim\n",
    "        self.hidden = hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.actor = Actor(self.encoder, self.dim, self.hidden, self.vocab_size)\n",
    "        self.mousenet = MouseNet(self.in_channels)\n",
    "        self.vision_rffn = ResidualFFN(self.mousenet.out.in_features, hidden, dim)\n",
    "        self.fusion = nn.MultiheadAttention(dim, num_heads=8)\n",
    "        self.pointer_head = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            self.mousenet.out,  # x, y normalisés\n",
    "            nn.Sigmoid()       # borné entre [0, 1] (si tu veux)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, tgt, screenshots=None):\n",
    "        \"\"\"\n",
    "        x: encoder output (batch_size, dim),\n",
    "\n",
    "        tgt: tensor of shape (batch_size, seq_len),\n",
    "\n",
    "        screenshot: image converti en niveau de gris,\n",
    "        \n",
    "        Retourne une liste de listes contenant les ID générés ainsi que des coordonnées (x, y)\n",
    "        \"\"\"\n",
    "        if screenshots is not None:\n",
    "            # encoder le text\n",
    "            txt_encoded = self.actor.rffn(x) # shape: (batch_size, dim)\n",
    "            txt_encoded = txt_encoded.unsqueeze(0) # shape: (1, batch_size, dim)\n",
    "            \n",
    "            # encoder l'image\n",
    "            vision_features = self.mousenet.forward(screenshots, features_only=True) # shape: (batch_size, 128)\n",
    "            vision_encoded = self.vision_rffn(vision_features) # shape: (batch_size, dim)\n",
    "            vision_encoded = vision_encoded.unsqueeze(0) # shape: (1, batch_size, dim)\n",
    "            fused, _ = self.fusion(txt_encoded, vision_encoded, vision_encoded) # shape (1, batch_size, dim)\n",
    "\n",
    "            # traiter target sequence\n",
    "            tgt = tgt.to(device)\n",
    "            tgt = self.actor.embedding(tgt)  # (batch_size, seq_len, dim)\n",
    "            tgt = tgt.permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "            tgt = self.actor.pos_encoding(tgt)  #  positional encoding\n",
    "\n",
    "            # Masque auto-régressif pour le décodeur\n",
    "            seq_len = tgt.size(0)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(tgt.device)\n",
    "\n",
    "            # passage au decoder\n",
    "            z = self.actor.transformer_decoder(tgt, fused, tgt_mask=tgt_mask)  # (seq_len, batch_size, dim)\n",
    "            z = self.actor.final_projection(z)  # (seq_len, batch_size, vocab_size)\n",
    "            z = z.permute(1, 0, 2)  # (batch_size, seq_len, vocab_size)\n",
    "            pointer_out = self.pointer_head(vision_encoded.squeeze(0))  # (batch_size, 2)\n",
    "\n",
    "            return z, pointer_out\n",
    "        else:\n",
    "            return self.actor.forward_training(x, tgt), screenshots\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x_text:list[str], screenshots=None, max_len=32, start_token_id=1, end_token_id=2):\n",
    "        \"\"\"\n",
    "        x_text : liste de string\n",
    "        Retourne une liste de listes contenant les ID générés\n",
    "        \"\"\"\n",
    "        if screenshots is not None:\n",
    "            # Encode input texts\n",
    "            with torch.no_grad():\n",
    "                txt_features = self.encoder.encode(x_text, convert_to_tensor=True) # shape: (batch_size, 384)\n",
    "            txt_encoded = self.actor.rffn(txt_features) # shape: (batch_size, dim)\n",
    "            txt_encoded = txt_encoded.unsqueeze(0) # shape: (1, batch_size, dim)\n",
    "\n",
    "            # encoder l'image\n",
    "            vision_features = self.mousenet.forward(screenshots, features_only=True) # shape: (batch_size, 128)\n",
    "            vision_encoded = self.vision_rffn(vision_features) # shape: (batch_size, dim)\n",
    "            vision_encoded = vision_encoded.unsqueeze(0) # shape: (1, batch_size, dim)\n",
    "            x, _ = self.fusion(txt_encoded, vision_encoded, vision_encoded) # shape (1, batch_size, dim)\n",
    "            pointer_out = self.pointer_head(vision_encoded.squeeze(0))  # (batch_size, 2)\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "            device = x.device\n",
    "\n",
    "            # Initialiser avec <BOS>\n",
    "            generated = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                # Embed + position\n",
    "                tgt_embed = self.actor.embedding(generated)  # (batch_size, seq_len, dim)\n",
    "                tgt_embed = tgt_embed.permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "                tgt_embed = self.actor.pos_encoding(tgt_embed)\n",
    "\n",
    "                # Masque causal\n",
    "                seq_len = generated.size(1)\n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
    "\n",
    "                # Decode\n",
    "                output = self.actor.transformer_decoder(tgt_embed, x, tgt_mask=tgt_mask)\n",
    "                logits = self.actor.final_projection(output)  # (seq_len, batch_size, vocab_size)\n",
    "                next_token_logits = logits[-1, :, :]  # dernier pas de temps → (batch_size, vocab_size)\n",
    "\n",
    "                # Greedy : choisir l'indice du max\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "                # Ajouter à la séquence\n",
    "                generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "                # Option d'arrêt : si tous les batchs ont généré <EOS>\n",
    "                if (next_token == end_token_id).all():\n",
    "                    break\n",
    "\n",
    "            return generated, pointer_out  # ((batch_size, seq_len_generated), (batch_size, 2))\n",
    "        else:\n",
    "            return self.actor.generate(x_text), screenshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ea6ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim, hidden, in_channels = 512, 512, 1\n",
    "model = VisionActor(in_channels, encoder, dim, hidden, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "921bf0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 1, 448, 448])\n",
      "tensor([[   1,  791,   12, 1603,    2]], device='mps:0')\n",
      "tensor([[0.5217, 0.5157]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "z, coordinate = model.generate([\"open safari\"], img_arr)\n",
    "print(z)\n",
    "print(coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57ea3bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: ['cmd+space', 'facetime', 'enter'],Pos mousse x: 0.5112466216087341, Pos mousse y: 0.5101023316383362\n"
     ]
    }
   ],
   "source": [
    "def translate(seq):\n",
    "    return [id_to_action[n] for n in seq.tolist() if n!=1 and n!=2 and n!=3]\n",
    "\n",
    "translation = translate(z[0])\n",
    "print(f\"sequence: {translation},\" + f\"Pos mousse x: {coordinate[0][0] if coordinate is not None else \"no entry\"}, Pos mousse y: {coordinate[0][1] if coordinate is not None else \"no entry\"}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
