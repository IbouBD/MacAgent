{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sentencepiece as spm\n",
    "from torch.utils.data import DataLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MacOSv1 , vocabsize = 1739\n",
    "MacOSv1 , vocabsize = 1785\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=50):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.encode('This is a test sentence', convert_to_tensor=True)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = '/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/vocab/macos_vocab.txt'\n",
    "DATA_PATH = '/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/dataset/dataset5-bis.txt'\n",
    "TokenIZER_PATH = '/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/notebook/tokenizer3.model'\n",
    "SPECIAL_KEYS_PATH = '/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/vocab/key_vocab.txt'\n",
    "OUTPUT_PATH = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/vocab/test2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Liste des touches clavier à rechercher\n",
    "KEYWORDS = [\"cmd\", \"ctrl\", \"shift\", \"alt\", \"option\", \"fn\", \"enter\", \"esc\", \"delete\", \"tab\", \"space\", \"capslock\", \"arrow\", \"+\"]\n",
    "\n",
    "def clean_quoted_tokens(text):\n",
    "    def replacer(match):\n",
    "        content = match.group(1)\n",
    "        if any(key in content.lower() for key in KEYWORDS):\n",
    "            return f\"'{content.replace(' ', '')}'\"\n",
    "        return match.group(0)  # ne pas modifier si aucune touche détectée\n",
    "    return re.sub(r\"'([^']*)'\", replacer, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des listes et dictionnaires\n",
    "l = []\n",
    "X = []\n",
    "Y = []\n",
    "dataset = []\n",
    "\"\"\"\n",
    "\n",
    "# Lecture du fichier\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    cleaned_lines = [clean_quoted_tokens(line) for line in f]\n",
    "    for line in cleaned_lines:\n",
    "        line = line.lower()\n",
    "        l.append(line.strip())\n",
    "\n",
    "for data in l:\n",
    "    split_data = data.split(\";\")\n",
    "    x = split_data[0]\n",
    "    y = split_data[1]\n",
    "    try:\n",
    "        y = y.replace(\"'cmd + space'\", \"'cmd+space'\")\n",
    "        y = y.replace(\"[\", \"\")\n",
    "        y = y.replace(\"]\", \"\")\n",
    "        y = y.replace(\",\", \"\")\n",
    "        y = y.replace(\"'\", \"\")\n",
    "        y = y.replace(\"[\", \"\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    dataset.append((x,y))\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\"\"\"\n",
    "\n",
    "app_path = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/vocab/app_vocab.txt\"\n",
    "instruction_path = \"/Users/ibrahimbaldediallo/Documents/Code/Jarvis_project/vocab/variation.txt\"\n",
    "\n",
    "instruction = []\n",
    "\n",
    "\n",
    "with open(instruction_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"-\"):\n",
    "            instruct = line.replace(\"- \", \"\")\n",
    "            instruction.append(instruct)\n",
    "\n",
    "with open(app_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for app in lines:\n",
    "        for instruct in instruction:\n",
    "            x = instruct.replace('###', app.strip())\n",
    "            y = f\"['cmd+space', '{app.strip()}', 'enter']\"\n",
    "            y = y.replace(\"[\", \"\")\n",
    "            y = y.replace(\"]\", \"\")\n",
    "            y = y.replace(\",\", \"\")\n",
    "            y = y.replace(\"'\", \"\")\n",
    "            y = y.replace(\"[\", \"\")\n",
    "            dataset.append((x, y))\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    \n",
    "print(dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y[0])\n",
    "print(Y[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for x,y in zip(X,Y):\n",
    "   word_listx = x.split()\n",
    "   for word in word_listx:\n",
    "      if word not in words:\n",
    "        words.append(word)\n",
    "   word_listy = y.split()\n",
    "   for word in word_listy:\n",
    "      if word not in words:\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def build_action_vocab(vocab_file, extra_words=None):\n",
    "    # Lire les lignes du vocabulaire fichier\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # Ajouter les tokens spéciaux et les mots supplémentaires éventuels\n",
    "    special_tokens = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "    all_tokens = special_tokens + vocab_lines+ words\n",
    "    if extra_words:\n",
    "        all_tokens += extra_words\n",
    "\n",
    "    # Supprimer les doublons tout en conservant l’ordre\n",
    "    all_tokens_unique = list(OrderedDict.fromkeys(all_tokens))\n",
    "\n",
    "    # Construire les dictionnaires\n",
    "    action_to_id = {token: idx for idx, token in enumerate(all_tokens_unique)}\n",
    "    id_to_action = {idx: token for token, idx in action_to_id.items()}\n",
    "\n",
    "    return action_to_id, id_to_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_id, id_to_action = build_action_vocab(VOCAB_PATH)\n",
    "print(len(action_to_id))\n",
    "print(action_to_id[\"safari\"])\n",
    "print(id_to_action[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(action_to_id)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "random.shuffle(X)\n",
    "random.shuffle(Y)\n",
    "\n",
    "X = X[:int(0.3*len(X))]\n",
    "Y = Y[:int(0.3*len(Y))]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_idx = []\n",
    "Y_idx_sentence = []\n",
    "for s in Y:\n",
    "    s = s.split()\n",
    "    s = [action_to_id[word] for word in s]\n",
    "    Y_idx.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Y_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "data = []\n",
    "PAD = action_to_id.get(\"<PAD>\", 0)\n",
    "BOS = action_to_id.get(\"<BOS>\", 1)\n",
    "EOS = action_to_id.get(\"<EOS>\", 2)\n",
    "UNK = action_to_id.get(\"<UNK>\", 3)\n",
    "\n",
    "decoder_inputs = []\n",
    "decoder_targets = []\n",
    "encoder_inputs = []\n",
    "\n",
    "# Construction brute des séquences\n",
    "for x, y in zip(X, Y_idx):\n",
    "    input_ids = model.encode(x, convert_to_tensor=True).to(device)\n",
    "    decoder_input = [BOS] + y\n",
    "    decoder_target = y + [EOS]\n",
    "\n",
    "    encoder_inputs.append(input_ids)\n",
    "    decoder_inputs.append(torch.tensor(decoder_input, dtype=torch.long))\n",
    "    decoder_targets.append(torch.tensor(decoder_target, dtype=torch.long))\n",
    "\n",
    "# Trouver la longueur max\n",
    "max_len = max(max(len(seq) for seq in decoder_inputs),\n",
    "              max(len(seq) for seq in decoder_targets))\n",
    "\n",
    "# Padding des séquences\n",
    "decoder_inputs_padded = pad_sequence(decoder_inputs, batch_first=True, padding_value=PAD)\n",
    "decoder_targets_padded = pad_sequence(decoder_targets, batch_first=True, padding_value=PAD)\n",
    "\n",
    "# Combine avec les entrées encodeur\n",
    "for i in range(len(X)):\n",
    "    data.append((encoder_inputs[i],\n",
    "                 (decoder_inputs_padded[i], decoder_targets_padded[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data[0]\n",
    "b = data[1]\n",
    "print(a[1][0].shape)\n",
    "print(b[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    encoder_batch = torch.stack([item[0] for item in batch])\n",
    "    decoder_input_batch = torch.stack([item[1][0] for item in batch])\n",
    "    decoder_target_batch = torch.stack([item[1][1] for item in batch])\n",
    "    return {\n",
    "        \"encoder_input\": encoder_batch,\n",
    "        \"decoder_input\": decoder_input_batch,\n",
    "        \"decoder_target\": decoder_target_batch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(\n",
    "    data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate_fn(batch)\n",
    ")\n",
    "\n",
    "# Exemple d'une itération\n",
    "for batch in dataloader:\n",
    "    encoder_input = batch[\"encoder_input\"]        # (B, D)\n",
    "    decoder_input = batch[\"decoder_input\"]        # (B, T)\n",
    "    decoder_target = batch[\"decoder_target\"]      # (B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequences, id_to_action, stop_token=\"<EOS>\"):\n",
    "    decoded_sequences = []\n",
    "    for sequence in sequences:\n",
    "        decoded = []\n",
    "        for idx in sequence:\n",
    "            token = id_to_action.get(idx, \"<UNK>\")\n",
    "            if token == stop_token:\n",
    "                break\n",
    "            decoded.append(token)\n",
    "        decoded_sequences.append(decoded)\n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_targets = decode(batch[\"decoder_target\"], id_to_action)\n",
    "print(batch[\"decoder_target\"].tolist())\n",
    "print(decoded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in batch[\"decoder_input\"][0].tolist():\n",
    "    print(idx, id_to_action.get(idx, \"<UNK>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "specials_keys = []\n",
    "with open(SPECIAL_KEYS_PATH, \"r\") as vocab:\n",
    "    for line in vocab:\n",
    "        if not line.startswith(\"#\"):\n",
    "            specials_keys.append(line.strip())\n",
    "\n",
    "print(specials_keys)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "w = []\n",
    "try:\n",
    "    with open(VOCAB_PATH, \"r\") as vocab, open(OUTPUT_PATH, \"w\") as output:\n",
    "        for line in vocab:\n",
    "            word = line.strip()  # Strip whitespace from the word\n",
    "            if word in specials_keys:\n",
    "                word = \"#\"\n",
    "            if not line.startswith(\"#\") and word != \"#\":\n",
    "                w.append(word)\n",
    "                output.write(word + \"\\n\")  # Write the word to the output file followed by a newline\n",
    "\n",
    "    print(f\"Filtered words have been written to {OUTPUT_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {VOCAB_PATH} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "key_list = [\n",
    "    # Commandes système\n",
    "    \"'cmd+space'\", \"'cmd+tab'\", \"'enter'\", \"'return'\", \"'tab'\", \"'esc'\",\n",
    "    # Apps\n",
    "    \"'safari'\", \"'chrome'\", \"'terminal'\", \"'finder'\",\n",
    "     # Symboles techniques (à traiter comme un seul token)\n",
    "    \"'[',\", \"', '\", \"']'\", \"' ▁'\", \"'['cmd+space', '\", \"'safari', '\", \"'enter']'\",\n",
    "    # Modificateurs\n",
    "    \"'cmd'\", \"'shift'\", \"'ctrl'\", \"'alt'\", \"'fn'\"\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "phase1_listx = []\n",
    "phase1_listy = []\n",
    "with open(phase1, \"r\") as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\"Input\"):\n",
    "            phase1_listx.append(line)\n",
    "        elif line.startswith(\"Output\"):\n",
    "            phase1_listy.append(line)\n",
    "        \n",
    "print(len(phase1_listx))\n",
    "print(len(phase1_listy))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=OUTPUT_PATH,\n",
    "    model_prefix='tokenizer3',\n",
    "    model_type='bpe',\n",
    "    vocab_size=1500,  # Légèrement augmenté\n",
    "    user_defined_symbols=key_list,\n",
    "    pad_id=3,\n",
    "    treat_whitespace_as_suffix=True,  # Nouveau\n",
    "    split_by_whitespace=False,  # Important\n",
    "    remove_extra_whitespaces=False\n",
    ")\n",
    "# Load trained tokenizer\n",
    "sp = spm.SentencePieceProcessor(model_file=TokenIZER_PATH)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"vocab_size = len(sp)\n",
    "print(f\"Vocab size: {vocab_size}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Test detokenization\n",
    "print(sp.Decode([2, 4,1452,26, 10]))  # Example output: \"▁Open ▁Alacritty ▁\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "special_tokens = [\"<s>\", \"<PAD>\", \"</s>\"]  # 0, 1, 2\n",
    "special_tokens_values = [sp.bos_id(), sp.pad_id(), sp.eos_id()]\n",
    "print(special_tokens[1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"x_tokenized = [\" \".join(sp.encode_as_pieces(seq)) for seq in X]\n",
    "y_tokenized = [\" \".join(sp.encode_as_pieces(seq)) for seq in Y]\n",
    "print(x_tokenized[0])\n",
    "print(y_tokenized[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def pad_and_convert_to_tensor(sequences, max_len=MAX_LEN, return_tensor=True):\n",
    "    \n",
    "    Encode text with SentencePiece, then pad/truncate to fixed length.\n",
    "    Ensures all output tensors have length max_len.\n",
    "    \n",
    "    pad_id = sp.pad_id()  \n",
    "\n",
    "    padded_sequences = []\n",
    "    if return_tensor:\n",
    "        for seq in sequences:\n",
    "            encoded = sp.Encode(seq, add_bos=True, add_eos=True, out_type=int)\n",
    "\n",
    "            # Troncature\n",
    "            encoded = encoded[:max_len - 1]\n",
    "\n",
    "            # Padding\n",
    "            while len(encoded) < max_len:\n",
    "                encoded.append(pad_id)\n",
    "\n",
    "            \n",
    "            padded_sequences.append(torch.tensor(encoded, dtype=torch.long).to(device))\n",
    "    else:\n",
    "            for seq in sequences:\n",
    "                tokens = seq.strip().split()\n",
    "\n",
    "                # Truncate if too long\n",
    "                tokens = tokens[:max_len - 1]\n",
    "\n",
    "                # Add <END>\n",
    "                tokens.append(\"</s>\")\n",
    "\n",
    "                # Pad if too short\n",
    "                while len(tokens) < max_len:\n",
    "                    tokens.append(\"<PAD>\")\n",
    "\n",
    "\n",
    "                padded_sequences.append(\" \".join(tokens))\n",
    "\n",
    "    return padded_sequences\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train = pad_and_convert_to_tensor(x_tokenized, return_tensor=False)\n",
    "y_train = pad_and_convert_to_tensor(Y, return_tensor=True)\n",
    "print(X[:2])\n",
    "print(x_train[:2])\n",
    "print()\n",
    "print(Y[:2])\n",
    "print(y_train[:2])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training_data = list(zip(x_train[:int(0.9*len(x_train))], y_train[:int(0.9*len(y_train))]))\n",
    "test_data = list(zip(x_train[int(0.9*len(x_train)):], y_train[int(0.9*len(y_train)):]))\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(test_data))\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(train_features[0])\n",
    "print(train_labels[0].tolist())\n",
    "print(sp.Decode(train_labels[0].tolist()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"embeddings = model.encode(X)\n",
    "embeddings = np.array(embeddings) #(n, 384)\n",
    "#embeddings = np.permute_dims(embeddings, (1, 0)) # Should be (384, n) \n",
    "print(embeddings.shape)  \n",
    "#print(embeddings)\n",
    "\n",
    "projection = umap.UMAP(n_neighbors=5, n_components=3).fit_transform(embeddings[:20])\n",
    "ax.scatter(projection[:, 0], projection[:, 1], projection[:, 2], marker='o')\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=2):\n",
    "        super(ResidualFFN, self).__init__()\n",
    "        \n",
    "        # Projection initiale\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Blocs résiduels\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Projection finale\n",
    "        self.output_proj = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Appliquer les blocs résiduels\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        return self.output_proj(x)\n",
    "        \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.layers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, dim)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "class MacOSActionModel(nn.Module):\n",
    "    def __init__(self, encoder, dim, hidden, vocab_size, max_len=128):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder  # pretrained SentenceTransformer\n",
    "        self.rffn = ResidualFFN(384, hidden, dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_encoding = PositionalEncoding(dim, max_len=max_len)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=dim, nhead=16, dim_feedforward=hidden, dropout=0.3)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=2)  # Réduction de 6 à 2 couches\n",
    "        self.final_projection = nn.Linear(dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        self.dim = dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x_texts, tgt):\n",
    "        \"\"\"\n",
    "        x_texts: list of strings, len = batch_size\n",
    "        tgt: tensor of shape (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = len(x_texts)\n",
    "        \n",
    "        # Encode input texts\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder.encode(x_texts, convert_to_tensor=True)  # shape: (batch_size, 384)\n",
    "        x = self.rffn(x)  # shape: (batch_size, dim)\n",
    "       \n",
    "\n",
    "        # Prepare target sequence\n",
    "        tgt = tgt.to(device)\n",
    "        tgt = self.embedding(tgt)  # (batch_size, seq_len, dim)\n",
    "        tgt = tgt.permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "        tgt = self.pos_encoding(tgt)  # add positional encoding\n",
    "\n",
    "        # Create mask for autoregressive decoding\n",
    "        seq_len = tgt.size(0)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(tgt.device)\n",
    "        x = x.unsqueeze(0).repeat(seq_len, 1, 1)  # (seq_len, batch_size, dim)\n",
    "        # Decode\n",
    "        z = self.transformer_decoder(tgt, x, tgt_mask=tgt_mask)  # (seq_len, batch_size, dim)\n",
    "        z = self.final_projection(z)  # (seq_len, batch_size, vocab_size)\n",
    "        z = z.permute(1, 0, 2)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def forward_training(self, x, tgt):\n",
    "        \"\"\"\n",
    "        x: encoder output (batch_size, dim)\n",
    "        tgt: tensor of shape (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # Projette x dans le bon espace si nécessaire\n",
    "        x = self.rffn(x)  # (batch_size, dim)\n",
    "\n",
    "        # Embedding + Positional encoding\n",
    "        tgt = self.embedding(tgt)  # (batch_size, seq_len, dim)\n",
    "        tgt = tgt.permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "\n",
    "        # Memory (encoder output) doit être (seq_len_enc, batch_size, dim)\n",
    "        # Ici on suppose x est global, donc on le répète\n",
    "        x = x.unsqueeze(0)  # (1, batch_size, dim)\n",
    "\n",
    "        # Masque auto-régressif pour le décodeur\n",
    "        seq_len = tgt.size(0)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(tgt.device)\n",
    "\n",
    "        # Transformer decoder\n",
    "        z = self.transformer_decoder(tgt, x, tgt_mask=tgt_mask)  # (seq_len, batch_size, dim)\n",
    "        z = self.final_projection(z)  # (seq_len, batch_size, vocab_size)\n",
    "        z = z.permute(1, 0, 2)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return z\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, x_text:list[str], max_len=32, start_token_id=1, end_token_id=2):\n",
    "        \"\"\"\n",
    "        x_text : liste de string\n",
    "        Retourne une liste de listes contenant les ID générés\n",
    "        \"\"\"\n",
    "        # Encode input texts\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder.encode(x_text, convert_to_tensor=True)\n",
    "        # Encoder: passe par rffn si nécessaire\n",
    "        x = self.rffn(x)  # (batch_size, dim)\n",
    "        memory = x.unsqueeze(0)  # (1, batch_size, dim)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        device = x.device\n",
    "\n",
    "        # Initialiser avec <BOS>\n",
    "        generated = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Embed + position\n",
    "            tgt_embed = self.embedding(generated)  # (batch_size, seq_len, dim)\n",
    "            tgt_embed = tgt_embed.permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "            tgt_embed = self.pos_encoding(tgt_embed)\n",
    "\n",
    "            # Masque causal\n",
    "            seq_len = generated.size(1)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
    "\n",
    "            # Decode\n",
    "            output = self.transformer_decoder(tgt_embed, memory, tgt_mask=tgt_mask)\n",
    "            logits = self.final_projection(output)  # (seq_len, batch_size, vocab_size)\n",
    "            next_token_logits = logits[-1, :, :]  # dernier pas de temps → (batch_size, vocab_size)\n",
    "\n",
    "            # Greedy : choisir l'indice du max\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "            # Ajouter à la séquence\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "            # Option d'arrêt : si tous les batchs ont généré <EOS>\n",
    "            if (next_token == end_token_id).all():\n",
    "                break\n",
    "\n",
    "        return generated  # (batch_size, seq_len_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim, hidden, = 512, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = MacOSActionModel(model, dim, hidden, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "p = actor(train_features[:2], train_labels[:2])\n",
    "print(\"target shape :\", train_labels[0].shape)\n",
    "print(\"Model output shape :\", p.shape)\n",
    "out = torch.argmax(p, dim=-1).squeeze(0).tolist()\n",
    "print(\"Model output indices :\", out)\n",
    "\n",
    "# Convert the indices to words\n",
    "decoded_words = sp.Decode(out)\n",
    "print(\"Model output :\",decoded_words)\n",
    "\n",
    "p = actor.generate([\"open safari\", \"open alacritty\"], max_length=20, beam_size=5)\n",
    "print(\"Model output shape :\", p.shape)\n",
    "print(\"Model output indices :\", sp.Decode(p[0].tolist()))\n",
    "print(\"Model output indices :\", sp.Decode(p[1].tolist()))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[\"encoder_input\"].shape)\n",
    "print(batch[\"decoder_target\"][0])\n",
    "print(batch[\"decoder_target\"][:, 1:][0])\n",
    "print(batch[\"decoder_input\"][0])\n",
    "print(batch[\"decoder_input\"][:, :-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = actor.forward_training(batch[\"encoder_input\"].to(device), batch[\"decoder_target\"].to(device))\n",
    "print(p)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_dataset):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  \n",
    "    total_loss = 0\n",
    "\n",
    "    for test_batch_x, test_batch_y in test_dataset:\n",
    "        test_batch_y = test_batch_y.to(device)\n",
    "        tgt_input = test_batch_y[:, :-1]\n",
    "        tgt_output = test_batch_y[:, 1:]\n",
    "\n",
    "        output = model(test_batch_x, tgt_input)\n",
    "        loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, test_dataset, epochs, learning_rate):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)     \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_dataset:\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            decoder_input = batch[\"decoder_input\"].to(device)\n",
    "            decoder_target = batch[\"decoder_target\"].to(device)\n",
    "            \n",
    "            # tgt_input : tout sauf le dernier token\n",
    "            decoder_input = decoder_input\n",
    "            # tgt_output : tout sauf le premier token (ce qu’on doit prédire)\n",
    "            decoder_target = decoder_target\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward_training(encoder_input, decoder_input)  # shape: (batch_size, seq_len, vocab_size)\n",
    "            \n",
    "            loss = criterion(output.reshape(-1, vocab_size), decoder_target.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataset)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        #val_loss = evaluate_model(model, test_dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        #print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(actor, dataloader, None, epochs=2, learning_rate=4e-5)  # best lr for now is 1e-4, 4e-5, 2e-5 avec 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save model\n",
    "torch.save(actor.state_dict(), 'pretrainded_actor.pth')\n",
    "\n",
    "with open('id_to_action.json', 'w') as fp:\n",
    "    json.dump(id_to_action, fp)\n",
    "    \n",
    "with open('action_to_id.json', 'w') as fp:\n",
    "    json.dump(action_to_id, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = actor.generate([\"open finder\"], max_len=32)\n",
    "print(\"Model output shape :\", p.shape)\n",
    "print(p)\n",
    "#print(\"Model output indices :\", clean_generation(sp.Decode(p[1].tolist())))\n",
    "#print(\"Model output indices :\", clean_generation(sp.Decode(p[-1].tolist())))\n",
    "print([id_to_action[n] for n in p[0].tolist() if n!=1 and n!=2 and n!=3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_state_dict(torch.load('pretrainded_actor.pth'))\n",
    "actor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = actor.generate([\"open weather\"], max_len=32)\n",
    "print(\"Model output shape :\", p.shape)\n",
    "print(p)\n",
    "#print(\"Model output indices :\", clean_generation(sp.Decode(p[1].tolist())))\n",
    "#print(\"Model output indices :\", clean_generation(sp.Decode(p[-1].tolist())))\n",
    "print([id_to_action[n] for n in p[0].tolist() if n!=1 and n!=2 and n!=3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
